{
 "metadata": {
  "name": "",
  "signature": "sha256:411e55e87174da1fc8991a5eb71552a65d7a546556bbe85d22eab7653aedaae0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Koller Scraper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from selenium import webdriver\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "import pandas as pd\n",
      "import math\n",
      "import re\n",
      "\n",
      "def getWholepage(url):\n",
      "    \n",
      "    driver = webdriver.Firefox()\n",
      "    driver.implicitly_wait(30)\n",
      "    driver.get(url)\n",
      "    html_source = driver.page_source\n",
      "    data = html_source.encode('utf-8')\n",
      "    \n",
      "    sp = BeautifulSoup(data)\n",
      "    for x in sp.find_all(\"option\", \"wp-filterselect wp-novalue\"):\n",
      "        num = re.sub(\"[^0-9]\", \"\", x.text)\n",
      "    \n",
      "    for i in range(0, int(num)/2):\n",
      "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
      "        time.sleep(1)\n",
      "\n",
      "    html_source = driver.page_source\n",
      "    data = html_source.encode('utf-8')\n",
      "    driver.quit()\n",
      "    return data\n",
      "    \n",
      "    \n",
      "def getPages(soup, name, num):\n",
      "        \n",
      "    fieldDict = {}\n",
      "    for x in range(0, num):\n",
      "        term = 'p' + str(x)\n",
      "        fieldDict[term] = []\n",
      "    fieldDict['img'] = []  \n",
      "    \n",
      "    for x in soup.find_all(\"a\", name):\n",
      "        if x['href'][:3] == '/en':\n",
      "            \n",
      "            pageURL = \"https://www.kollerauktionen.ch\" + x['href']\n",
      "            newdriver = webdriver.Firefox()\n",
      "            newdriver.implicitly_wait(30)\n",
      "            newdriver.get(pageURL)\n",
      "            html_source = newdriver.page_source\n",
      "            data = html_source.encode('utf-8')\n",
      "            sp = BeautifulSoup(data)\n",
      "            \n",
      "            info = []\n",
      "            for x in sp.find_all('p'):\n",
      "                info.append(x.text)         \n",
      "            for x in sp.find_all('img', 'img-responsive'):\n",
      "                fieldDict['img'].append(\"https://www.kollerauktionen.ch\" + x['data-large-image'])\n",
      "                break\n",
      "            for x in range(0, num):\n",
      "                term = 'p' + str(x)\n",
      "                fieldDict[term].append(info[x])\n",
      "            newdriver.quit()\n",
      "              \n",
      "    return fieldDict\n",
      "        \n",
      "\n",
      "def parseCategory(url):\n",
      "    \n",
      "    page = getWholepage(url)\n",
      "    soup = BeautifulSoup(page)\n",
      "\n",
      "    attrClass = \"M0200_Action_Panel btn btn-default btn-xs\"\n",
      "    fieldNum = 12\n",
      "    data = getPages(soup, attrClass, fieldNum)\n",
      "    \n",
      "    df = pd.DataFrame()\n",
      "    for x in data:\n",
      "        df[x] = data[x]\n",
      "    \n",
      "    df.to_csv( url.split('/')[-1]+ '.csv', encoding='utf-8', index=False)\n",
      "    \n",
      "\n",
      "if __name__ == \"__main__\":    \n",
      "    \n",
      "    driver = webdriver.Firefox()\n",
      "    driver.implicitly_wait(30)\n",
      "\n",
      "    driver.get(\"https://www.kollerauktionen.ch/en/auctioncalendar-archive.htm\")\n",
      "    html_source = driver.page_source\n",
      "    data = html_source.encode('utf-8')\n",
      "    soup = BeautifulSoup(data)\n",
      "\n",
      "    parseCategory(\"https://www.kollerauktionen.ch/en/genf/g57/wine-2\")\n",
      "    \n",
      "#     i = 0\n",
      "#     for x in soup.find_all(\"a\", \"btn btn-default btn-xs\"):\n",
      "#         if x.text == 'Online catalogue':\n",
      "#             parseCategory(\"https://www.kollerauktionen.ch\" + x['href'])\n",
      "#             i += 1\n",
      "         #   parseCategory(\"https://www.kollerauktionen.ch/en/zuerich/a170/19th-century-paintings-8\")\n",
      "         #   break\n",
      "            \n",
      "    driver.quit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Antiquorum Scraper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import cookielib\n",
      "import mechanize\n",
      "from mechanize._opener import urlopen\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "br = mechanize.Browser()\n",
      "cj = cookielib.LWPCookieJar()\n",
      "br.set_cookiejar(cj)\n",
      "\n",
      "br.set_handle_equiv(True)\n",
      "br.set_handle_gzip(True)\n",
      "br.set_handle_redirect(True)\n",
      "br.set_handle_referer(True)\n",
      "br.set_handle_robots(False)\n",
      "\n",
      "nums = []\n",
      "title = []\n",
      "num = 0\n",
      "\n",
      "br = mechanize.Browser()\n",
      "\n",
      "for x in range(1, 15):\n",
      "    response = br.open(\"http://www.antiquorum.com/price-list/?page=\" + str(x))\n",
      "    soup = BeautifulSoup(response.read())\n",
      "    for y in soup.find_all('a', 'view-btn'):\n",
      "\n",
      "        newres = br.open(y['href'])\n",
      "        sp =  BeautifulSoup(newres.read())\n",
      "        for a in sp.find_all('td', 'gborder') :\n",
      "            try:\n",
      "                if 'javascript' in a.a['href']:\n",
      "                    num += 1\n",
      "            except:\n",
      "                continue\n",
      "        for b in sp.find_all('td', 'searchcount'):\n",
      "#            print b\n",
      "            if len(b.text) > 0 and 'ersion' not in b.text and 'No lots sold for this Auction' not in b.text:\n",
      "#                print b.text\n",
      "                title.append(b.text)\n",
      "        nums.append(num)\n",
      "        num = 0\n",
      "    \n",
      "#     print len(title), len(nums), x\n",
      "#     title = []\n",
      "#     nums = [] \n",
      "#     num = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "np.save('antiquorum_titles.npy', title)\n",
      "np.save('antiquorum_numbers.npy', nums)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Aguttes Scraper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import cookielib\n",
      "import mechanize\n",
      "from mechanize._opener import urlopen\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "br = mechanize.Browser()\n",
      "cj = cookielib.LWPCookieJar()\n",
      "br.set_cookiejar(cj)\n",
      "\n",
      "br.set_handle_equiv(True)\n",
      "br.set_handle_gzip(True)\n",
      "br.set_handle_redirect(True)\n",
      "br.set_handle_referer(True)\n",
      "br.set_handle_robots(False)\n",
      "\n",
      "times = []\n",
      "titles = []\n",
      "nums = []\n",
      "\n",
      "br = mechanize.Browser()\n",
      "\n",
      "for ye in range(2006, 2017):\n",
      "    response = br.open(\"http://www.aguttes.com/html/calendrier.jsp?t=p&a=\" + str(ye) + \"&ligne=&chmod=false\")\n",
      "    soup = BeautifulSoup(response.read())\n",
      "    for x in soup.find_all('div', 'col-md-10 entry-c'):\n",
      "        try:\n",
      "\n",
      "            for z in x.find_all('div', 'entry-title'):     \n",
      "                res = br.open('http://www.aguttes.com' + z.a['href'])\n",
      "                sp = BeautifulSoup(res.read())\n",
      "                for y in sp.find_all('div', 'nbre_lot_haut'):\n",
      "                    num = y.text\n",
      "                \n",
      "#                 for a in sp.find_all('div', 'ordre_false product clearfix'):\n",
      "#                     if a['id'][:3] == 'lot':\n",
      "#                         num += 1\n",
      "#                 titles.append(z.text)     \n",
      "                \n",
      "            for y in x.find_all('div', 'bloc_vente_date'):\n",
      "                times.append(y.text)\n",
      "            \n",
      "            nums.append(num)\n",
      "            num = ''\n",
      "        except:\n",
      "            continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "#np.save('Aguttes_titles', titles)\n",
      "np.save('Aguttes_times', times)\n",
      "np.save('Aguttes_numbers', nums)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}